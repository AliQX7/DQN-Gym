{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "197090c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e19cb5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cf9104",
   "metadata": {},
   "source": [
    "We use the Keras RL2 library for the DQN and its training. We import our Cart Pole environment as well from the OpenAI Gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da28d372",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "actions = env.action_space.n # Action space, the number of actions we can take in each state, this is 2 (Left, Right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "937346c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 4)                 0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                80        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 34        \n",
      "                                                                 \n",
      " activation (Activation)     (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 658\n",
      "Trainable params: 658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,env.observation_space.shape[0]))) # Our state space is 4 (pos,vel,angle,rotation)\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4a5ae6",
   "metadata": {},
   "source": [
    "Our model is simple with 3 hidden dense layers and an input and output layer. The input layer takes our state space as input which consists of (Cart position, Cart velocity, Angle of pole, Rate of rotation of pole). This model then outputs either an action to the Left or Right for the cart.\n",
    "3 Dense layers is enough complexity for our problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b850b4cd",
   "metadata": {},
   "source": [
    "We now define our policy, the Epsilon Greedy Q Policy which picks the highest expected reward. \n",
    "The Sequential Memory helps us store past experiences of our agent so that it may learn from them and use them in subsequent episodes. \n",
    "We then make our agent, compile our model and then fit the DQN over 50,000 episodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9737f6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "    11/50000: episode: 1, duration: 0.056s, episode steps:  11, steps per second: 197, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --\n",
      "    21/50000: episode: 2, duration: 0.006s, episode steps:  10, steps per second: 1756, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: --, mae: --, mean_q: --\n",
      "    31/50000: episode: 3, duration: 0.007s, episode steps:  10, steps per second: 1382, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --\n",
      "    40/50000: episode: 4, duration: 0.006s, episode steps:   9, steps per second: 1502, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --\n",
      "    50/50000: episode: 5, duration: 0.006s, episode steps:  10, steps per second: 1665, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: --, mae: --, mean_q: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "C:\\Users\\Ali\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    59/50000: episode: 6, duration: 0.496s, episode steps:   9, steps per second:  18, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.490008, mae: 0.537189, mean_q: 0.336151\n",
      "    68/50000: episode: 7, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.383163, mae: 0.494420, mean_q: 0.471404\n",
      "    79/50000: episode: 8, duration: 0.068s, episode steps:  11, steps per second: 163, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.279492, mae: 0.470645, mean_q: 0.655427\n",
      "    89/50000: episode: 9, duration: 0.061s, episode steps:  10, steps per second: 164, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.199859, mae: 0.451024, mean_q: 0.861373\n",
      "    98/50000: episode: 10, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.157536, mae: 0.431498, mean_q: 1.072256\n",
      "   107/50000: episode: 11, duration: 0.051s, episode steps:   9, steps per second: 175, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.155716, mae: 0.467130, mean_q: 1.297238\n",
      "   115/50000: episode: 12, duration: 0.050s, episode steps:   8, steps per second: 160, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.157220, mae: 0.484550, mean_q: 1.319804\n",
      "   124/50000: episode: 13, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.149824, mae: 0.517818, mean_q: 1.337572\n",
      "   132/50000: episode: 14, duration: 0.052s, episode steps:   8, steps per second: 155, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.124072, mae: 0.531909, mean_q: 1.420767\n",
      "   141/50000: episode: 15, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.111835, mae: 0.543238, mean_q: 1.534923\n",
      "   150/50000: episode: 16, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.113112, mae: 0.570624, mean_q: 1.587444\n",
      "   160/50000: episode: 17, duration: 0.062s, episode steps:  10, steps per second: 161, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.111947, mae: 0.608537, mean_q: 1.608562\n",
      "   169/50000: episode: 18, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.094525, mae: 0.601671, mean_q: 1.743792\n",
      "   181/50000: episode: 19, duration: 0.077s, episode steps:  12, steps per second: 157, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.086740, mae: 0.639633, mean_q: 1.817683\n",
      "   189/50000: episode: 20, duration: 0.050s, episode steps:   8, steps per second: 161, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.087179, mae: 0.676349, mean_q: 1.849214\n",
      "   198/50000: episode: 21, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.104199, mae: 0.726542, mean_q: 1.883376\n",
      "   208/50000: episode: 22, duration: 0.064s, episode steps:  10, steps per second: 157, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.084547, mae: 0.748891, mean_q: 1.966928\n",
      "   218/50000: episode: 23, duration: 0.061s, episode steps:  10, steps per second: 163, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.088502, mae: 0.786568, mean_q: 2.014486\n",
      "   228/50000: episode: 24, duration: 0.061s, episode steps:  10, steps per second: 163, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.085742, mae: 0.824325, mean_q: 2.043865\n",
      "   237/50000: episode: 25, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.087402, mae: 0.849637, mean_q: 2.096856\n",
      "   251/50000: episode: 26, duration: 0.088s, episode steps:  14, steps per second: 160, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.143 [0.000, 1.000],  loss: 0.085849, mae: 0.878821, mean_q: 2.166370\n",
      "   260/50000: episode: 27, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.098988, mae: 0.928495, mean_q: 2.132526\n",
      "   269/50000: episode: 28, duration: 0.052s, episode steps:   9, steps per second: 174, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.083648, mae: 0.973086, mean_q: 2.314856\n",
      "   279/50000: episode: 29, duration: 0.056s, episode steps:  10, steps per second: 177, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.090638, mae: 1.003033, mean_q: 2.292554\n",
      "   288/50000: episode: 30, duration: 0.054s, episode steps:   9, steps per second: 168, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.096481, mae: 1.048472, mean_q: 2.316158\n",
      "   297/50000: episode: 31, duration: 0.053s, episode steps:   9, steps per second: 169, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.074864, mae: 1.105127, mean_q: 2.459544\n",
      "   307/50000: episode: 32, duration: 0.055s, episode steps:  10, steps per second: 182, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.091673, mae: 1.148476, mean_q: 2.405125\n",
      "   317/50000: episode: 33, duration: 0.061s, episode steps:  10, steps per second: 163, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.082457, mae: 1.188296, mean_q: 2.466894\n",
      "   327/50000: episode: 34, duration: 0.066s, episode steps:  10, steps per second: 152, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.088662, mae: 1.242376, mean_q: 2.516520\n",
      "   339/50000: episode: 35, duration: 0.076s, episode steps:  12, steps per second: 159, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.083 [0.000, 1.000],  loss: 0.069233, mae: 1.282175, mean_q: 2.634598\n",
      "   351/50000: episode: 36, duration: 0.072s, episode steps:  12, steps per second: 166, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.086885, mae: 1.339017, mean_q: 2.639825\n",
      "   361/50000: episode: 37, duration: 0.058s, episode steps:  10, steps per second: 173, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.078403, mae: 1.384954, mean_q: 2.713545\n",
      "   370/50000: episode: 38, duration: 0.053s, episode steps:   9, steps per second: 169, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.052097, mae: 1.410196, mean_q: 2.804153\n",
      "   381/50000: episode: 39, duration: 0.062s, episode steps:  11, steps per second: 179, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.080050, mae: 1.460877, mean_q: 2.819340\n",
      "   391/50000: episode: 40, duration: 0.057s, episode steps:  10, steps per second: 177, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.067434, mae: 1.477151, mean_q: 2.866430\n",
      "   399/50000: episode: 41, duration: 0.046s, episode steps:   8, steps per second: 173, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.125 [0.000, 1.000],  loss: 0.071789, mae: 1.497027, mean_q: 2.867510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   409/50000: episode: 42, duration: 0.057s, episode steps:  10, steps per second: 177, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.061579, mae: 1.579082, mean_q: 3.081443\n",
      "   419/50000: episode: 43, duration: 0.058s, episode steps:  10, steps per second: 171, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.070510, mae: 1.588602, mean_q: 3.014932\n",
      "   430/50000: episode: 44, duration: 0.062s, episode steps:  11, steps per second: 178, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.063069, mae: 1.619445, mean_q: 3.127813\n",
      "   439/50000: episode: 45, duration: 0.051s, episode steps:   9, steps per second: 178, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.055716, mae: 1.644322, mean_q: 3.196111\n",
      "   450/50000: episode: 46, duration: 0.063s, episode steps:  11, steps per second: 174, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.057961, mae: 1.664667, mean_q: 3.210552\n",
      "   459/50000: episode: 47, duration: 0.051s, episode steps:   9, steps per second: 175, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.050926, mae: 1.732479, mean_q: 3.365521\n",
      "   471/50000: episode: 48, duration: 0.071s, episode steps:  12, steps per second: 170, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.074696, mae: 1.725203, mean_q: 3.265693\n",
      "   482/50000: episode: 49, duration: 0.072s, episode steps:  11, steps per second: 153, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.068908, mae: 1.766157, mean_q: 3.398942\n",
      "   494/50000: episode: 50, duration: 0.078s, episode steps:  12, steps per second: 155, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.057950, mae: 1.819821, mean_q: 3.504575\n",
      "   543/50000: episode: 51, duration: 0.260s, episode steps:  49, steps per second: 188, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.612 [0.000, 1.000],  loss: 0.063942, mae: 1.962542, mean_q: 3.744316\n",
      "   590/50000: episode: 52, duration: 0.248s, episode steps:  47, steps per second: 189, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.596 [0.000, 1.000],  loss: 0.088647, mae: 2.086046, mean_q: 3.968347\n",
      "   643/50000: episode: 53, duration: 0.282s, episode steps:  53, steps per second: 188, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 0.122316, mae: 2.331903, mean_q: 4.456524\n",
      "   655/50000: episode: 54, duration: 0.067s, episode steps:  12, steps per second: 180, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.206705, mae: 2.453220, mean_q: 4.680459\n",
      "   664/50000: episode: 55, duration: 0.051s, episode steps:   9, steps per second: 177, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.151763, mae: 2.513707, mean_q: 4.835707\n",
      "   715/50000: episode: 56, duration: 0.274s, episode steps:  51, steps per second: 186, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 0.227405, mae: 2.626761, mean_q: 5.013202\n",
      "   724/50000: episode: 57, duration: 0.050s, episode steps:   9, steps per second: 179, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.156148, mae: 2.884292, mean_q: 5.643140\n",
      "   735/50000: episode: 58, duration: 0.061s, episode steps:  11, steps per second: 181, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.365875, mae: 2.860484, mean_q: 5.402844\n",
      "   765/50000: episode: 59, duration: 0.160s, episode steps:  30, steps per second: 188, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.296574, mae: 2.893513, mean_q: 5.544229\n",
      "   783/50000: episode: 60, duration: 0.097s, episode steps:  18, steps per second: 186, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 0.404214, mae: 3.028075, mean_q: 5.772020\n",
      "   815/50000: episode: 61, duration: 0.183s, episode steps:  32, steps per second: 174, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 0.401687, mae: 3.186090, mean_q: 6.093448\n",
      "   829/50000: episode: 62, duration: 0.084s, episode steps:  14, steps per second: 166, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.181014, mae: 3.228509, mean_q: 6.233488\n",
      "   890/50000: episode: 63, duration: 0.342s, episode steps:  61, steps per second: 178, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 0.488672, mae: 3.371440, mean_q: 6.447761\n",
      "   950/50000: episode: 64, duration: 0.337s, episode steps:  60, steps per second: 178, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.486956, mae: 3.554531, mean_q: 6.827852\n",
      "  1038/50000: episode: 65, duration: 0.469s, episode steps:  88, steps per second: 188, episode reward: 88.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 0.529962, mae: 3.891969, mean_q: 7.503170\n",
      "  1056/50000: episode: 66, duration: 0.099s, episode steps:  18, steps per second: 182, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.561348, mae: 4.039393, mean_q: 7.764671\n",
      "  1078/50000: episode: 67, duration: 0.118s, episode steps:  22, steps per second: 187, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.455446, mae: 4.092514, mean_q: 7.946141\n",
      "  1119/50000: episode: 68, duration: 0.224s, episode steps:  41, steps per second: 183, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.561 [0.000, 1.000],  loss: 0.433928, mae: 4.168355, mean_q: 8.110600\n",
      "  1226/50000: episode: 69, duration: 0.556s, episode steps: 107, steps per second: 193, episode reward: 107.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 0.673616, mae: 4.451093, mean_q: 8.595071\n",
      "  1337/50000: episode: 70, duration: 0.593s, episode steps: 111, steps per second: 187, episode reward: 111.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 0.732053, mae: 4.788329, mean_q: 9.208261\n",
      "  1537/50000: episode: 71, duration: 1.035s, episode steps: 200, steps per second: 193, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 0.575585, mae: 5.295924, mean_q: 10.297140\n",
      "  1737/50000: episode: 72, duration: 1.030s, episode steps: 200, steps per second: 194, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 0.634149, mae: 6.042092, mean_q: 11.850227\n",
      "  1867/50000: episode: 73, duration: 0.668s, episode steps: 130, steps per second: 195, episode reward: 130.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.478638, mae: 6.649300, mean_q: 13.171231\n",
      "  2067/50000: episode: 74, duration: 1.029s, episode steps: 200, steps per second: 194, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 0.852675, mae: 7.252117, mean_q: 14.357285\n",
      "  2263/50000: episode: 75, duration: 1.002s, episode steps: 196, steps per second: 196, episode reward: 196.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 0.950538, mae: 7.979399, mean_q: 15.836226\n",
      "  2463/50000: episode: 76, duration: 1.023s, episode steps: 200, steps per second: 196, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 0.985409, mae: 8.803653, mean_q: 17.611366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2663/50000: episode: 77, duration: 1.027s, episode steps: 200, steps per second: 195, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 1.491327, mae: 9.556177, mean_q: 19.047083\n",
      "  2863/50000: episode: 78, duration: 1.031s, episode steps: 200, steps per second: 194, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 1.174830, mae: 10.299348, mean_q: 20.609648\n",
      "  3063/50000: episode: 79, duration: 1.028s, episode steps: 200, steps per second: 194, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 1.320414, mae: 11.044594, mean_q: 22.161993\n",
      "  3263/50000: episode: 80, duration: 1.029s, episode steps: 200, steps per second: 194, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 1.644631, mae: 11.845074, mean_q: 23.712879\n",
      "  3463/50000: episode: 81, duration: 1.026s, episode steps: 200, steps per second: 195, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 1.897103, mae: 12.568572, mean_q: 25.073650\n",
      "  3663/50000: episode: 82, duration: 1.027s, episode steps: 200, steps per second: 195, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 1.725441, mae: 13.179048, mean_q: 26.410273\n",
      "  3863/50000: episode: 83, duration: 1.028s, episode steps: 200, steps per second: 194, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 1.622696, mae: 13.935349, mean_q: 27.907578\n",
      "  4063/50000: episode: 84, duration: 1.030s, episode steps: 200, steps per second: 194, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 1.777713, mae: 14.545698, mean_q: 29.208319\n",
      "  4263/50000: episode: 85, duration: 1.031s, episode steps: 200, steps per second: 194, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 2.642177, mae: 15.297490, mean_q: 30.580391\n",
      "  4463/50000: episode: 86, duration: 1.033s, episode steps: 200, steps per second: 194, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 2.064010, mae: 15.909411, mean_q: 31.923767\n",
      "  4663/50000: episode: 87, duration: 1.028s, episode steps: 200, steps per second: 195, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 2.726557, mae: 16.558741, mean_q: 33.120438\n",
      "  4863/50000: episode: 88, duration: 1.034s, episode steps: 200, steps per second: 193, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 2.582518, mae: 17.201284, mean_q: 34.496407\n",
      "  5063/50000: episode: 89, duration: 1.028s, episode steps: 200, steps per second: 195, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 2.736611, mae: 17.903858, mean_q: 35.931870\n",
      "  5263/50000: episode: 90, duration: 1.032s, episode steps: 200, steps per second: 194, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 2.156237, mae: 18.485752, mean_q: 37.102680\n",
      "  5463/50000: episode: 91, duration: 1.026s, episode steps: 200, steps per second: 195, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 3.124156, mae: 19.140596, mean_q: 38.288223\n",
      "  5663/50000: episode: 92, duration: 1.031s, episode steps: 200, steps per second: 194, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 3.778916, mae: 19.647041, mean_q: 39.195908\n",
      "  5863/50000: episode: 93, duration: 1.026s, episode steps: 200, steps per second: 195, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 2.670342, mae: 20.177275, mean_q: 40.388435\n",
      "  6063/50000: episode: 94, duration: 1.029s, episode steps: 200, steps per second: 194, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 2.588135, mae: 20.734428, mean_q: 41.506172\n",
      "  6263/50000: episode: 95, duration: 1.021s, episode steps: 200, steps per second: 196, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 2.490397, mae: 21.360186, mean_q: 42.822968\n",
      "  6463/50000: episode: 96, duration: 1.036s, episode steps: 200, steps per second: 193, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 2.480666, mae: 22.127110, mean_q: 44.337543\n",
      "  6663/50000: episode: 97, duration: 1.026s, episode steps: 200, steps per second: 195, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 4.026734, mae: 22.460522, mean_q: 44.816547\n",
      "  6863/50000: episode: 98, duration: 1.031s, episode steps: 200, steps per second: 194, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 4.439392, mae: 22.965942, mean_q: 45.789917\n",
      "  7063/50000: episode: 99, duration: 1.035s, episode steps: 200, steps per second: 193, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 3.657450, mae: 23.348433, mean_q: 46.751888\n",
      "  7263/50000: episode: 100, duration: 1.033s, episode steps: 200, steps per second: 194, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 3.920031, mae: 23.801916, mean_q: 47.586632\n",
      "  7463/50000: episode: 101, duration: 1.032s, episode steps: 200, steps per second: 194, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 3.616787, mae: 24.453590, mean_q: 48.864796\n",
      "  7663/50000: episode: 102, duration: 1.033s, episode steps: 200, steps per second: 194, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 5.241626, mae: 24.905540, mean_q: 49.686665\n",
      "  7863/50000: episode: 103, duration: 1.025s, episode steps: 200, steps per second: 195, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 5.017848, mae: 25.293013, mean_q: 50.395000\n",
      "  8063/50000: episode: 104, duration: 1.026s, episode steps: 200, steps per second: 195, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 4.372883, mae: 25.656162, mean_q: 51.246338\n",
      "  8263/50000: episode: 105, duration: 1.031s, episode steps: 200, steps per second: 194, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 3.522570, mae: 26.170792, mean_q: 52.337143\n",
      "  8463/50000: episode: 106, duration: 1.028s, episode steps: 200, steps per second: 195, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 3.598526, mae: 26.541389, mean_q: 53.132343\n",
      "  8663/50000: episode: 107, duration: 1.029s, episode steps: 200, steps per second: 194, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 4.099161, mae: 27.003691, mean_q: 54.061447\n",
      "  8863/50000: episode: 108, duration: 1.022s, episode steps: 200, steps per second: 196, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 3.004073, mae: 27.501484, mean_q: 55.068886\n",
      "  9063/50000: episode: 109, duration: 1.025s, episode steps: 200, steps per second: 195, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 4.078321, mae: 28.035112, mean_q: 56.065918\n",
      "  9263/50000: episode: 110, duration: 1.026s, episode steps: 200, steps per second: 195, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 4.626526, mae: 28.394770, mean_q: 56.723278\n",
      "  9463/50000: episode: 111, duration: 0.972s, episode steps: 200, steps per second: 206, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 5.101746, mae: 28.825468, mean_q: 57.589722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9663/50000: episode: 112, duration: 0.969s, episode steps: 200, steps per second: 206, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 3.423090, mae: 29.242886, mean_q: 58.498619\n",
      "  9863/50000: episode: 113, duration: 0.967s, episode steps: 200, steps per second: 207, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 4.987478, mae: 29.704840, mean_q: 59.382275\n",
      " 10063/50000: episode: 114, duration: 0.982s, episode steps: 200, steps per second: 204, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 3.576928, mae: 29.898775, mean_q: 59.929787\n",
      " 10263/50000: episode: 115, duration: 0.976s, episode steps: 200, steps per second: 205, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 5.108745, mae: 30.498447, mean_q: 61.002262\n",
      " 10463/50000: episode: 116, duration: 0.974s, episode steps: 200, steps per second: 205, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 5.733459, mae: 30.750616, mean_q: 61.513058\n",
      " 10663/50000: episode: 117, duration: 0.972s, episode steps: 200, steps per second: 206, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 4.852682, mae: 31.361319, mean_q: 62.809669\n",
      " 10863/50000: episode: 118, duration: 0.981s, episode steps: 200, steps per second: 204, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 4.341769, mae: 31.599480, mean_q: 63.203426\n",
      " 11063/50000: episode: 119, duration: 0.966s, episode steps: 200, steps per second: 207, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 4.511099, mae: 31.882271, mean_q: 63.816261\n",
      " 11263/50000: episode: 120, duration: 0.998s, episode steps: 200, steps per second: 200, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 5.005318, mae: 32.003960, mean_q: 64.056229\n",
      " 11460/50000: episode: 121, duration: 0.984s, episode steps: 197, steps per second: 200, episode reward: 197.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 4.367340, mae: 32.225433, mean_q: 64.557106\n",
      " 11660/50000: episode: 122, duration: 0.991s, episode steps: 200, steps per second: 202, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 3.535952, mae: 32.533123, mean_q: 65.274857\n",
      " 11860/50000: episode: 123, duration: 0.995s, episode steps: 200, steps per second: 201, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 5.802228, mae: 32.996159, mean_q: 65.983879\n",
      " 12060/50000: episode: 124, duration: 1.009s, episode steps: 200, steps per second: 198, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 5.049678, mae: 32.990429, mean_q: 66.022293\n",
      " 12260/50000: episode: 125, duration: 1.006s, episode steps: 200, steps per second: 199, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 4.372566, mae: 33.223022, mean_q: 66.607658\n",
      " 12460/50000: episode: 126, duration: 1.012s, episode steps: 200, steps per second: 198, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 3.990535, mae: 33.338566, mean_q: 66.829445\n",
      " 12660/50000: episode: 127, duration: 1.020s, episode steps: 200, steps per second: 196, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.174941, mae: 33.750904, mean_q: 67.645287\n",
      " 12860/50000: episode: 128, duration: 0.965s, episode steps: 200, steps per second: 207, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 4.223625, mae: 34.048660, mean_q: 68.365173\n",
      " 13060/50000: episode: 129, duration: 0.974s, episode steps: 200, steps per second: 205, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 6.333169, mae: 34.184181, mean_q: 68.508026\n",
      " 13260/50000: episode: 130, duration: 0.971s, episode steps: 200, steps per second: 206, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 3.439835, mae: 34.495987, mean_q: 69.146751\n",
      " 13460/50000: episode: 131, duration: 0.970s, episode steps: 200, steps per second: 206, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 3.147970, mae: 34.593761, mean_q: 69.364647\n",
      " 13660/50000: episode: 132, duration: 0.972s, episode steps: 200, steps per second: 206, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 3.894719, mae: 34.670357, mean_q: 69.450638\n",
      " 13860/50000: episode: 133, duration: 0.982s, episode steps: 200, steps per second: 204, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 3.925124, mae: 34.876377, mean_q: 69.854172\n",
      " 14060/50000: episode: 134, duration: 0.964s, episode steps: 200, steps per second: 208, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 5.127036, mae: 35.145668, mean_q: 70.316254\n",
      " 14260/50000: episode: 135, duration: 0.972s, episode steps: 200, steps per second: 206, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 4.544744, mae: 35.117134, mean_q: 70.390160\n",
      " 14460/50000: episode: 136, duration: 0.993s, episode steps: 200, steps per second: 202, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 3.521260, mae: 35.060665, mean_q: 70.415001\n",
      " 14660/50000: episode: 137, duration: 0.993s, episode steps: 200, steps per second: 201, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 3.735543, mae: 35.211254, mean_q: 70.629082\n",
      " 14860/50000: episode: 138, duration: 0.995s, episode steps: 200, steps per second: 201, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 3.955453, mae: 34.987133, mean_q: 70.229240\n",
      " 15060/50000: episode: 139, duration: 0.978s, episode steps: 200, steps per second: 204, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 4.138332, mae: 35.374100, mean_q: 70.909027\n",
      " 15260/50000: episode: 140, duration: 0.970s, episode steps: 200, steps per second: 206, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 3.571756, mae: 35.297695, mean_q: 70.803535\n",
      " 15460/50000: episode: 141, duration: 0.994s, episode steps: 200, steps per second: 201, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.308072, mae: 35.452206, mean_q: 71.115471\n",
      " 15646/50000: episode: 142, duration: 0.915s, episode steps: 186, steps per second: 203, episode reward: 186.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 3.875736, mae: 35.416996, mean_q: 71.034370\n",
      " 15846/50000: episode: 143, duration: 0.998s, episode steps: 200, steps per second: 200, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 4.355915, mae: 35.445911, mean_q: 71.023605\n",
      " 16046/50000: episode: 144, duration: 1.015s, episode steps: 200, steps per second: 197, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 3.452421, mae: 35.684059, mean_q: 71.604485\n",
      " 16246/50000: episode: 145, duration: 1.000s, episode steps: 200, steps per second: 200, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 4.409274, mae: 35.623066, mean_q: 71.371994\n",
      " 16446/50000: episode: 146, duration: 0.993s, episode steps: 200, steps per second: 201, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 3.714699, mae: 35.643791, mean_q: 71.511406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16646/50000: episode: 147, duration: 1.012s, episode steps: 200, steps per second: 198, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 4.053677, mae: 35.493999, mean_q: 71.202522\n",
      " 16846/50000: episode: 148, duration: 1.004s, episode steps: 200, steps per second: 199, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 2.313276, mae: 35.730099, mean_q: 71.751579\n",
      " 17046/50000: episode: 149, duration: 0.991s, episode steps: 200, steps per second: 202, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 3.197711, mae: 35.917561, mean_q: 72.014847\n",
      " 17246/50000: episode: 150, duration: 1.007s, episode steps: 200, steps per second: 199, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 3.489936, mae: 35.709126, mean_q: 71.572601\n",
      " 17446/50000: episode: 151, duration: 0.991s, episode steps: 200, steps per second: 202, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 5.244806, mae: 35.976013, mean_q: 72.029221\n",
      " 17646/50000: episode: 152, duration: 0.972s, episode steps: 200, steps per second: 206, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 3.338315, mae: 36.157196, mean_q: 72.433517\n",
      " 17846/50000: episode: 153, duration: 0.973s, episode steps: 200, steps per second: 205, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 2.930583, mae: 35.857811, mean_q: 71.904251\n",
      " 18046/50000: episode: 154, duration: 1.005s, episode steps: 200, steps per second: 199, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 1.748390, mae: 35.745392, mean_q: 71.676598\n",
      " 18242/50000: episode: 155, duration: 0.983s, episode steps: 196, steps per second: 199, episode reward: 196.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 3.578217, mae: 36.320618, mean_q: 72.629204\n",
      " 18442/50000: episode: 156, duration: 1.021s, episode steps: 200, steps per second: 196, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 2.616890, mae: 36.239052, mean_q: 72.602524\n",
      " 18642/50000: episode: 157, duration: 1.008s, episode steps: 200, steps per second: 198, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 2.623461, mae: 35.996353, mean_q: 71.987236\n",
      " 18842/50000: episode: 158, duration: 1.034s, episode steps: 200, steps per second: 193, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 2.470920, mae: 36.073380, mean_q: 72.140144\n",
      " 19042/50000: episode: 159, duration: 1.177s, episode steps: 200, steps per second: 170, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 2.797191, mae: 35.803123, mean_q: 71.714409\n",
      " 19242/50000: episode: 160, duration: 1.407s, episode steps: 200, steps per second: 142, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 3.316608, mae: 35.835533, mean_q: 71.616623\n",
      " 19442/50000: episode: 161, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 3.405058, mae: 35.773911, mean_q: 71.530457\n",
      " 19642/50000: episode: 162, duration: 1.179s, episode steps: 200, steps per second: 170, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.204391, mae: 35.572319, mean_q: 71.222069\n",
      " 19842/50000: episode: 163, duration: 1.158s, episode steps: 200, steps per second: 173, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 2.219390, mae: 35.762230, mean_q: 71.481834\n",
      " 20042/50000: episode: 164, duration: 1.162s, episode steps: 200, steps per second: 172, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 2.534110, mae: 35.797340, mean_q: 71.653740\n",
      " 20242/50000: episode: 165, duration: 1.157s, episode steps: 200, steps per second: 173, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 1.805007, mae: 35.914856, mean_q: 71.963875\n",
      " 20442/50000: episode: 166, duration: 1.163s, episode steps: 200, steps per second: 172, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 3.230407, mae: 35.886776, mean_q: 71.749504\n",
      " 20642/50000: episode: 167, duration: 1.151s, episode steps: 200, steps per second: 174, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 2.955113, mae: 35.801926, mean_q: 71.533630\n",
      " 20842/50000: episode: 168, duration: 1.185s, episode steps: 200, steps per second: 169, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 1.795528, mae: 36.138248, mean_q: 72.359184\n",
      " 21042/50000: episode: 169, duration: 1.203s, episode steps: 200, steps per second: 166, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 2.441230, mae: 35.942638, mean_q: 71.891396\n",
      " 21242/50000: episode: 170, duration: 1.155s, episode steps: 200, steps per second: 173, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 2.500563, mae: 36.141685, mean_q: 72.265854\n",
      " 21442/50000: episode: 171, duration: 1.153s, episode steps: 200, steps per second: 174, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.006849, mae: 35.994278, mean_q: 71.962532\n",
      " 21642/50000: episode: 172, duration: 1.152s, episode steps: 200, steps per second: 174, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 2.800414, mae: 36.105396, mean_q: 72.137291\n",
      " 21842/50000: episode: 173, duration: 1.149s, episode steps: 200, steps per second: 174, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 3.384304, mae: 35.840137, mean_q: 71.610619\n",
      " 22042/50000: episode: 174, duration: 1.161s, episode steps: 200, steps per second: 172, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 2.317273, mae: 35.888676, mean_q: 71.737526\n",
      " 22242/50000: episode: 175, duration: 1.153s, episode steps: 200, steps per second: 173, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 2.416043, mae: 36.056412, mean_q: 72.090721\n",
      " 22442/50000: episode: 176, duration: 1.142s, episode steps: 200, steps per second: 175, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 3.353503, mae: 36.127579, mean_q: 72.168678\n",
      " 22642/50000: episode: 177, duration: 1.140s, episode steps: 200, steps per second: 175, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.231989, mae: 36.077438, mean_q: 72.119843\n",
      " 22842/50000: episode: 178, duration: 1.137s, episode steps: 200, steps per second: 176, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 3.787076, mae: 36.465790, mean_q: 72.833595\n",
      " 23042/50000: episode: 179, duration: 1.148s, episode steps: 200, steps per second: 174, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 3.039865, mae: 36.651810, mean_q: 73.253571\n",
      " 23242/50000: episode: 180, duration: 1.142s, episode steps: 200, steps per second: 175, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 2.778736, mae: 36.339737, mean_q: 72.731873\n",
      " 23442/50000: episode: 181, duration: 1.147s, episode steps: 200, steps per second: 174, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 5.324125, mae: 36.308479, mean_q: 72.287888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 23642/50000: episode: 182, duration: 1.146s, episode steps: 200, steps per second: 175, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 4.265272, mae: 36.622791, mean_q: 73.061073\n",
      " 23842/50000: episode: 183, duration: 1.146s, episode steps: 200, steps per second: 174, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 3.096060, mae: 36.329411, mean_q: 72.531837\n",
      " 24042/50000: episode: 184, duration: 1.171s, episode steps: 200, steps per second: 171, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.201906, mae: 36.306339, mean_q: 72.475662\n",
      " 24242/50000: episode: 185, duration: 1.198s, episode steps: 200, steps per second: 167, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 3.098291, mae: 36.370148, mean_q: 72.640930\n",
      " 24442/50000: episode: 186, duration: 1.144s, episode steps: 200, steps per second: 175, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 3.718647, mae: 36.330318, mean_q: 72.460457\n",
      " 24642/50000: episode: 187, duration: 1.152s, episode steps: 200, steps per second: 174, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 3.492132, mae: 36.284332, mean_q: 72.448288\n",
      " 24842/50000: episode: 188, duration: 1.144s, episode steps: 200, steps per second: 175, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 2.854805, mae: 36.554447, mean_q: 72.963631\n",
      " 25042/50000: episode: 189, duration: 1.144s, episode steps: 200, steps per second: 175, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 4.549612, mae: 36.513893, mean_q: 72.965744\n",
      " 25242/50000: episode: 190, duration: 1.149s, episode steps: 200, steps per second: 174, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 4.368457, mae: 36.321102, mean_q: 72.574532\n",
      " 25442/50000: episode: 191, duration: 1.155s, episode steps: 200, steps per second: 173, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 3.027042, mae: 36.240158, mean_q: 72.411629\n",
      " 25642/50000: episode: 192, duration: 1.155s, episode steps: 200, steps per second: 173, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 2.668920, mae: 36.002205, mean_q: 71.943863\n",
      " 25842/50000: episode: 193, duration: 1.149s, episode steps: 200, steps per second: 174, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 5.140703, mae: 36.351875, mean_q: 72.525711\n",
      " 26042/50000: episode: 194, duration: 1.144s, episode steps: 200, steps per second: 175, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 5.564756, mae: 36.747238, mean_q: 73.389015\n",
      " 26242/50000: episode: 195, duration: 1.147s, episode steps: 200, steps per second: 174, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 3.939893, mae: 36.529255, mean_q: 72.948898\n",
      " 26415/50000: episode: 196, duration: 0.994s, episode steps: 173, steps per second: 174, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 5.187183, mae: 36.731655, mean_q: 73.277832\n",
      " 26615/50000: episode: 197, duration: 1.150s, episode steps: 200, steps per second: 174, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 3.872111, mae: 36.746010, mean_q: 73.377274\n",
      " 26815/50000: episode: 198, duration: 1.161s, episode steps: 200, steps per second: 172, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 2.107035, mae: 36.998966, mean_q: 73.981102\n",
      " 27015/50000: episode: 199, duration: 1.148s, episode steps: 200, steps per second: 174, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 4.682829, mae: 36.608532, mean_q: 73.023590\n",
      " 27215/50000: episode: 200, duration: 1.166s, episode steps: 200, steps per second: 171, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 4.568679, mae: 36.430565, mean_q: 72.655327\n",
      " 27415/50000: episode: 201, duration: 1.201s, episode steps: 200, steps per second: 167, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 2.713020, mae: 36.325821, mean_q: 72.714691\n",
      " 27615/50000: episode: 202, duration: 1.172s, episode steps: 200, steps per second: 171, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 2.818728, mae: 36.492714, mean_q: 72.960289\n",
      " 27815/50000: episode: 203, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.989745, mae: 36.781208, mean_q: 73.414864\n",
      " 28015/50000: episode: 204, duration: 1.207s, episode steps: 200, steps per second: 166, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.537508, mae: 36.716934, mean_q: 73.271515\n",
      " 28215/50000: episode: 205, duration: 1.184s, episode steps: 200, steps per second: 169, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 5.888307, mae: 36.788074, mean_q: 73.390472\n",
      " 28415/50000: episode: 206, duration: 1.171s, episode steps: 200, steps per second: 171, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 4.508204, mae: 36.528549, mean_q: 72.959267\n",
      " 28615/50000: episode: 207, duration: 1.191s, episode steps: 200, steps per second: 168, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 3.956048, mae: 36.609528, mean_q: 72.956123\n",
      " 28815/50000: episode: 208, duration: 1.182s, episode steps: 200, steps per second: 169, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.838736, mae: 36.329884, mean_q: 72.491806\n",
      " 29015/50000: episode: 209, duration: 1.191s, episode steps: 200, steps per second: 168, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 3.993686, mae: 36.645226, mean_q: 73.202415\n",
      " 29215/50000: episode: 210, duration: 1.179s, episode steps: 200, steps per second: 170, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 5.049260, mae: 36.577999, mean_q: 72.980598\n",
      " 29415/50000: episode: 211, duration: 1.175s, episode steps: 200, steps per second: 170, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 6.312402, mae: 36.688915, mean_q: 73.208359\n",
      " 29614/50000: episode: 212, duration: 1.169s, episode steps: 199, steps per second: 170, episode reward: 199.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 4.877359, mae: 36.435009, mean_q: 72.744774\n",
      " 29814/50000: episode: 213, duration: 1.190s, episode steps: 200, steps per second: 168, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 4.355851, mae: 36.436249, mean_q: 72.824043\n",
      " 30014/50000: episode: 214, duration: 1.169s, episode steps: 200, steps per second: 171, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.128867, mae: 36.578522, mean_q: 73.095047\n",
      " 30214/50000: episode: 215, duration: 1.202s, episode steps: 200, steps per second: 166, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 5.831648, mae: 36.708641, mean_q: 73.266266\n",
      " 30414/50000: episode: 216, duration: 1.170s, episode steps: 200, steps per second: 171, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 4.496144, mae: 36.807652, mean_q: 73.423386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 30614/50000: episode: 217, duration: 1.172s, episode steps: 200, steps per second: 171, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 4.017107, mae: 36.477436, mean_q: 72.767021\n",
      " 30814/50000: episode: 218, duration: 1.194s, episode steps: 200, steps per second: 167, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 4.270726, mae: 36.429497, mean_q: 72.662621\n",
      " 31014/50000: episode: 219, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 6.412231, mae: 36.631336, mean_q: 72.935959\n",
      " 31214/50000: episode: 220, duration: 1.318s, episode steps: 200, steps per second: 152, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 5.278707, mae: 36.386211, mean_q: 72.473625\n",
      " 31414/50000: episode: 221, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 5.338218, mae: 36.545658, mean_q: 72.892975\n",
      " 31614/50000: episode: 222, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.180163, mae: 36.593227, mean_q: 73.001526\n",
      " 31814/50000: episode: 223, duration: 1.318s, episode steps: 200, steps per second: 152, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 4.579510, mae: 36.640034, mean_q: 73.033791\n",
      " 32014/50000: episode: 224, duration: 1.341s, episode steps: 200, steps per second: 149, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 3.760291, mae: 36.558990, mean_q: 73.015427\n",
      " 32214/50000: episode: 225, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 5.334172, mae: 36.594673, mean_q: 72.986343\n",
      " 32406/50000: episode: 226, duration: 1.178s, episode steps: 192, steps per second: 163, episode reward: 192.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 4.857964, mae: 36.667179, mean_q: 73.232330\n",
      " 32606/50000: episode: 227, duration: 1.192s, episode steps: 200, steps per second: 168, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 7.398063, mae: 36.686195, mean_q: 73.188576\n",
      " 32743/50000: episode: 228, duration: 0.831s, episode steps: 137, steps per second: 165, episode reward: 137.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 8.746655, mae: 36.742817, mean_q: 73.224792\n",
      " 32905/50000: episode: 229, duration: 0.938s, episode steps: 162, steps per second: 173, episode reward: 162.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 2.243817, mae: 36.385506, mean_q: 72.777534\n",
      " 33045/50000: episode: 230, duration: 0.822s, episode steps: 140, steps per second: 170, episode reward: 140.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 2.641129, mae: 36.910702, mean_q: 73.921623\n",
      " 33181/50000: episode: 231, duration: 0.794s, episode steps: 136, steps per second: 171, episode reward: 136.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 7.540344, mae: 36.587532, mean_q: 73.067169\n",
      " 33371/50000: episode: 232, duration: 1.130s, episode steps: 190, steps per second: 168, episode reward: 190.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 4.909947, mae: 36.541203, mean_q: 72.981865\n",
      " 33571/50000: episode: 233, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 5.904057, mae: 36.836910, mean_q: 73.590256\n",
      " 33727/50000: episode: 234, duration: 0.995s, episode steps: 156, steps per second: 157, episode reward: 156.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 3.851284, mae: 36.918163, mean_q: 73.757385\n",
      " 33917/50000: episode: 235, duration: 1.215s, episode steps: 190, steps per second: 156, episode reward: 190.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 4.918660, mae: 36.392258, mean_q: 72.722389\n",
      " 34117/50000: episode: 236, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 3.894644, mae: 36.630074, mean_q: 73.201950\n",
      " 34317/50000: episode: 237, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 4.233647, mae: 36.406528, mean_q: 72.689163\n",
      " 34517/50000: episode: 238, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.361844, mae: 36.376366, mean_q: 72.626198\n",
      " 34712/50000: episode: 239, duration: 1.233s, episode steps: 195, steps per second: 158, episode reward: 195.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 5.663878, mae: 36.811096, mean_q: 73.485794\n",
      " 34912/50000: episode: 240, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 5.773307, mae: 36.422230, mean_q: 72.823967\n",
      " 35112/50000: episode: 241, duration: 1.164s, episode steps: 200, steps per second: 172, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 3.885286, mae: 36.729721, mean_q: 73.424118\n",
      " 35255/50000: episode: 242, duration: 0.871s, episode steps: 143, steps per second: 164, episode reward: 143.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 4.793619, mae: 36.929443, mean_q: 73.796509\n",
      " 35427/50000: episode: 243, duration: 1.041s, episode steps: 172, steps per second: 165, episode reward: 172.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 4.916262, mae: 36.345642, mean_q: 72.655617\n",
      " 35627/50000: episode: 244, duration: 1.194s, episode steps: 200, steps per second: 168, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 5.636162, mae: 36.673599, mean_q: 73.237686\n",
      " 35768/50000: episode: 245, duration: 0.846s, episode steps: 141, steps per second: 167, episode reward: 141.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 3.996819, mae: 36.243137, mean_q: 72.607780\n",
      " 35909/50000: episode: 246, duration: 0.966s, episode steps: 141, steps per second: 146, episode reward: 141.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 4.314217, mae: 36.135651, mean_q: 72.232567\n",
      " 36023/50000: episode: 247, duration: 0.774s, episode steps: 114, steps per second: 147, episode reward: 114.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 7.320775, mae: 36.439938, mean_q: 72.636856\n",
      " 36223/50000: episode: 248, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 4.137004, mae: 36.184666, mean_q: 72.308533\n",
      " 36423/50000: episode: 249, duration: 1.216s, episode steps: 200, steps per second: 165, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 5.055860, mae: 36.175362, mean_q: 72.259781\n",
      " 36623/50000: episode: 250, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 3.433363, mae: 36.241211, mean_q: 72.456985\n",
      " 36669/50000: episode: 251, duration: 0.285s, episode steps:  46, steps per second: 161, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 8.579385, mae: 36.595158, mean_q: 72.994392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 36869/50000: episode: 252, duration: 1.183s, episode steps: 200, steps per second: 169, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 5.616623, mae: 36.468143, mean_q: 72.853745\n",
      " 37069/50000: episode: 253, duration: 1.200s, episode steps: 200, steps per second: 167, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 5.747537, mae: 36.220020, mean_q: 72.289108\n",
      " 37225/50000: episode: 254, duration: 1.042s, episode steps: 156, steps per second: 150, episode reward: 156.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 7.285596, mae: 36.079193, mean_q: 72.075493\n",
      " 37425/50000: episode: 255, duration: 1.214s, episode steps: 200, steps per second: 165, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 6.399741, mae: 35.947330, mean_q: 71.739090\n",
      " 37625/50000: episode: 256, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 5.520276, mae: 35.819313, mean_q: 71.501564\n",
      " 37825/50000: episode: 257, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 4.009451, mae: 36.068104, mean_q: 72.153702\n",
      " 38025/50000: episode: 258, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 5.276567, mae: 35.575222, mean_q: 71.074913\n",
      " 38182/50000: episode: 259, duration: 0.985s, episode steps: 157, steps per second: 159, episode reward: 157.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 5.262336, mae: 35.996174, mean_q: 71.941917\n",
      " 38313/50000: episode: 260, duration: 0.836s, episode steps: 131, steps per second: 157, episode reward: 131.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 7.636999, mae: 35.834908, mean_q: 71.485764\n",
      " 38513/50000: episode: 261, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 6.282657, mae: 35.912285, mean_q: 71.786652\n",
      " 38700/50000: episode: 262, duration: 1.197s, episode steps: 187, steps per second: 156, episode reward: 187.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 2.821810, mae: 35.460976, mean_q: 71.006973\n",
      " 38900/50000: episode: 263, duration: 1.329s, episode steps: 200, steps per second: 150, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.628435, mae: 36.052441, mean_q: 72.050026\n",
      " 39100/50000: episode: 264, duration: 1.370s, episode steps: 200, steps per second: 146, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 4.740469, mae: 35.655071, mean_q: 71.270973\n",
      " 39300/50000: episode: 265, duration: 1.339s, episode steps: 200, steps per second: 149, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 4.911860, mae: 35.663124, mean_q: 71.204338\n",
      " 39500/50000: episode: 266, duration: 1.429s, episode steps: 200, steps per second: 140, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 3.858791, mae: 35.748547, mean_q: 71.435036\n",
      " 39700/50000: episode: 267, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 4.808654, mae: 35.809784, mean_q: 71.491898\n",
      " 39832/50000: episode: 268, duration: 0.792s, episode steps: 132, steps per second: 167, episode reward: 132.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 5.604509, mae: 35.663879, mean_q: 71.215744\n",
      " 40001/50000: episode: 269, duration: 1.015s, episode steps: 169, steps per second: 167, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 3.886132, mae: 35.641354, mean_q: 71.212326\n",
      " 40201/50000: episode: 270, duration: 1.207s, episode steps: 200, steps per second: 166, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.252985, mae: 35.598183, mean_q: 71.042450\n",
      " 40401/50000: episode: 271, duration: 1.347s, episode steps: 200, steps per second: 148, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 5.405511, mae: 35.612724, mean_q: 71.188133\n",
      " 40601/50000: episode: 272, duration: 1.342s, episode steps: 200, steps per second: 149, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 4.884242, mae: 35.602211, mean_q: 71.049011\n",
      " 40801/50000: episode: 273, duration: 1.211s, episode steps: 200, steps per second: 165, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 4.220105, mae: 35.678589, mean_q: 71.373230\n",
      " 40988/50000: episode: 274, duration: 1.132s, episode steps: 187, steps per second: 165, episode reward: 187.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 3.834319, mae: 35.750996, mean_q: 71.487373\n",
      " 41127/50000: episode: 275, duration: 0.851s, episode steps: 139, steps per second: 163, episode reward: 139.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 4.106425, mae: 35.747978, mean_q: 71.543518\n",
      " 41306/50000: episode: 276, duration: 1.084s, episode steps: 179, steps per second: 165, episode reward: 179.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 4.598784, mae: 35.720703, mean_q: 71.442726\n",
      " 41506/50000: episode: 277, duration: 1.208s, episode steps: 200, steps per second: 166, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 5.782000, mae: 35.909340, mean_q: 71.764549\n",
      " 41706/50000: episode: 278, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.832296, mae: 35.770172, mean_q: 71.626526\n",
      " 41906/50000: episode: 279, duration: 1.323s, episode steps: 200, steps per second: 151, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.189955, mae: 35.988209, mean_q: 71.955589\n",
      " 42106/50000: episode: 280, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 6.666149, mae: 36.112221, mean_q: 72.226768\n",
      " 42246/50000: episode: 281, duration: 0.835s, episode steps: 140, steps per second: 168, episode reward: 140.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 4.570220, mae: 36.028355, mean_q: 72.108070\n",
      " 42427/50000: episode: 282, duration: 1.069s, episode steps: 181, steps per second: 169, episode reward: 181.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 5.232714, mae: 36.261982, mean_q: 72.482742\n",
      " 42592/50000: episode: 283, duration: 0.985s, episode steps: 165, steps per second: 168, episode reward: 165.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 5.200180, mae: 36.162819, mean_q: 72.368576\n",
      " 42760/50000: episode: 284, duration: 1.058s, episode steps: 168, steps per second: 159, episode reward: 168.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 6.838715, mae: 36.225838, mean_q: 72.438141\n",
      " 42952/50000: episode: 285, duration: 1.212s, episode steps: 192, steps per second: 158, episode reward: 192.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 4.808043, mae: 36.322250, mean_q: 72.707573\n",
      " 43111/50000: episode: 286, duration: 0.960s, episode steps: 159, steps per second: 166, episode reward: 159.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 7.219773, mae: 36.277458, mean_q: 72.560440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 43265/50000: episode: 287, duration: 0.934s, episode steps: 154, steps per second: 165, episode reward: 154.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 7.257774, mae: 36.390633, mean_q: 72.767372\n",
      " 43465/50000: episode: 288, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.155315, mae: 36.490818, mean_q: 73.062813\n",
      " 43632/50000: episode: 289, duration: 1.032s, episode steps: 167, steps per second: 162, episode reward: 167.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 7.654600, mae: 36.549721, mean_q: 73.137062\n",
      " 43775/50000: episode: 290, duration: 0.913s, episode steps: 143, steps per second: 157, episode reward: 143.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 3.738839, mae: 36.164364, mean_q: 72.569046\n",
      " 43975/50000: episode: 291, duration: 1.205s, episode steps: 200, steps per second: 166, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 4.975979, mae: 36.380699, mean_q: 72.824203\n",
      " 44144/50000: episode: 292, duration: 1.010s, episode steps: 169, steps per second: 167, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 7.496923, mae: 36.494499, mean_q: 72.965759\n",
      " 44287/50000: episode: 293, duration: 0.878s, episode steps: 143, steps per second: 163, episode reward: 143.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 4.909306, mae: 36.566139, mean_q: 73.159691\n",
      " 44423/50000: episode: 294, duration: 0.821s, episode steps: 136, steps per second: 166, episode reward: 136.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 5.685215, mae: 36.526920, mean_q: 73.080078\n",
      " 44570/50000: episode: 295, duration: 0.899s, episode steps: 147, steps per second: 164, episode reward: 147.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 6.171814, mae: 36.896664, mean_q: 73.921539\n",
      " 44725/50000: episode: 296, duration: 0.975s, episode steps: 155, steps per second: 159, episode reward: 155.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 5.492705, mae: 37.077236, mean_q: 74.329811\n",
      " 44862/50000: episode: 297, duration: 0.814s, episode steps: 137, steps per second: 168, episode reward: 137.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.470966, mae: 36.878338, mean_q: 74.118797\n",
      " 45062/50000: episode: 298, duration: 1.190s, episode steps: 200, steps per second: 168, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 3.709063, mae: 37.169853, mean_q: 74.577980\n",
      " 45204/50000: episode: 299, duration: 0.907s, episode steps: 142, steps per second: 157, episode reward: 142.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 5.574075, mae: 37.318966, mean_q: 74.769836\n",
      " 45367/50000: episode: 300, duration: 1.074s, episode steps: 163, steps per second: 152, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 7.183402, mae: 37.534576, mean_q: 75.111382\n",
      " 45523/50000: episode: 301, duration: 0.942s, episode steps: 156, steps per second: 166, episode reward: 156.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 5.389603, mae: 37.258224, mean_q: 74.677750\n",
      " 45661/50000: episode: 302, duration: 0.818s, episode steps: 138, steps per second: 169, episode reward: 138.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2.968681, mae: 37.401550, mean_q: 75.060135\n",
      " 45804/50000: episode: 303, duration: 0.846s, episode steps: 143, steps per second: 169, episode reward: 143.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 4.547975, mae: 37.815041, mean_q: 75.752525\n",
      " 45962/50000: episode: 304, duration: 0.943s, episode steps: 158, steps per second: 168, episode reward: 158.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 10.905071, mae: 37.820534, mean_q: 75.501495\n",
      " 46123/50000: episode: 305, duration: 1.001s, episode steps: 161, steps per second: 161, episode reward: 161.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 5.341251, mae: 37.801468, mean_q: 75.686020\n",
      " 46271/50000: episode: 306, duration: 0.893s, episode steps: 148, steps per second: 166, episode reward: 148.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 5.914714, mae: 38.189972, mean_q: 76.484619\n",
      " 46443/50000: episode: 307, duration: 1.032s, episode steps: 172, steps per second: 167, episode reward: 172.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 5.310613, mae: 38.351048, mean_q: 76.856651\n",
      " 46643/50000: episode: 308, duration: 1.462s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 6.663418, mae: 38.498104, mean_q: 76.970154\n",
      " 46791/50000: episode: 309, duration: 0.912s, episode steps: 148, steps per second: 162, episode reward: 148.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 10.150906, mae: 38.603363, mean_q: 77.080521\n",
      " 46926/50000: episode: 310, duration: 0.826s, episode steps: 135, steps per second: 163, episode reward: 135.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 7.256182, mae: 38.851532, mean_q: 77.703796\n",
      " 47063/50000: episode: 311, duration: 0.840s, episode steps: 137, steps per second: 163, episode reward: 137.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 8.157084, mae: 38.641479, mean_q: 77.357635\n",
      " 47189/50000: episode: 312, duration: 0.870s, episode steps: 126, steps per second: 145, episode reward: 126.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 6.037627, mae: 38.818043, mean_q: 77.811684\n",
      " 47372/50000: episode: 313, duration: 1.248s, episode steps: 183, steps per second: 147, episode reward: 183.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 9.712134, mae: 38.763462, mean_q: 77.366943\n",
      " 47509/50000: episode: 314, duration: 0.863s, episode steps: 137, steps per second: 159, episode reward: 137.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 10.980465, mae: 38.743145, mean_q: 77.335945\n",
      " 47638/50000: episode: 315, duration: 0.756s, episode steps: 129, steps per second: 171, episode reward: 129.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 5.144509, mae: 38.976540, mean_q: 78.077248\n",
      " 47768/50000: episode: 316, duration: 0.794s, episode steps: 130, steps per second: 164, episode reward: 130.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 6.159271, mae: 39.037777, mean_q: 78.063972\n",
      " 47914/50000: episode: 317, duration: 0.926s, episode steps: 146, steps per second: 158, episode reward: 146.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 4.968759, mae: 38.964966, mean_q: 78.005913\n",
      " 48076/50000: episode: 318, duration: 0.999s, episode steps: 162, steps per second: 162, episode reward: 162.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 4.532987, mae: 38.906071, mean_q: 77.908600\n",
      " 48215/50000: episode: 319, duration: 0.859s, episode steps: 139, steps per second: 162, episode reward: 139.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 5.080914, mae: 39.682968, mean_q: 79.548492\n",
      " 48343/50000: episode: 320, duration: 0.797s, episode steps: 128, steps per second: 161, episode reward: 128.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 8.449228, mae: 39.628044, mean_q: 79.275177\n",
      " 48493/50000: episode: 321, duration: 0.992s, episode steps: 150, steps per second: 151, episode reward: 150.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 9.215917, mae: 39.790581, mean_q: 79.458275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 48619/50000: episode: 322, duration: 0.781s, episode steps: 126, steps per second: 161, episode reward: 126.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 6.516081, mae: 39.964394, mean_q: 80.030144\n",
      " 48762/50000: episode: 323, duration: 0.898s, episode steps: 143, steps per second: 159, episode reward: 143.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 5.480647, mae: 39.705761, mean_q: 79.496971\n",
      " 48898/50000: episode: 324, duration: 0.841s, episode steps: 136, steps per second: 162, episode reward: 136.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 9.340918, mae: 39.956284, mean_q: 79.848572\n",
      " 49053/50000: episode: 325, duration: 0.986s, episode steps: 155, steps per second: 157, episode reward: 155.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 5.108930, mae: 40.029346, mean_q: 80.231750\n",
      " 49184/50000: episode: 326, duration: 0.793s, episode steps: 131, steps per second: 165, episode reward: 131.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 6.050422, mae: 39.853050, mean_q: 79.872543\n",
      " 49298/50000: episode: 327, duration: 0.708s, episode steps: 114, steps per second: 161, episode reward: 114.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.916758, mae: 40.022488, mean_q: 80.032349\n",
      " 49477/50000: episode: 328, duration: 1.062s, episode steps: 179, steps per second: 169, episode reward: 179.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 6.040112, mae: 40.072811, mean_q: 80.299561\n",
      " 49612/50000: episode: 329, duration: 0.796s, episode steps: 135, steps per second: 170, episode reward: 135.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 7.319965, mae: 40.543831, mean_q: 81.136818\n",
      " 49751/50000: episode: 330, duration: 0.823s, episode steps: 139, steps per second: 169, episode reward: 139.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 5.576651, mae: 40.542965, mean_q: 81.115601\n",
      " 49923/50000: episode: 331, duration: 1.021s, episode steps: 172, steps per second: 169, episode reward: 172.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 7.594738, mae: 40.346794, mean_q: 80.642052\n",
      "done, took 286.341 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25ec13be280>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=50000, window_length=1) # Data structure to store agents past experiences\n",
    "dqn = DQNAgent(model=model, nb_actions=actions, memory=memory, nb_steps_warmup=50, target_model_update=0.01, policy=policy)\n",
    "dqn.compile(Adam(learning_rate=0.001), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c415ca9",
   "metadata": {},
   "source": [
    "We can now visualize our DQN Agent and see the reward we get. An average reward of above 195 means \n",
    "that our problem is considered \"solved\".\n",
    "We can also see from our training that it took about 2000 episodes for our DQN to converge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e37c9c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 200.000, steps: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25ec13be640>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30448901",
   "metadata": {},
   "source": [
    "We run and visualize our model by running the code above and see that our agent has learned to balance the pole. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c522d0db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
